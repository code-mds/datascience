{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loan repay prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data location: https://drive.switch.ch/index.php/s/pCy5ctcFRsM2RdZ\n",
    "\n",
    "Consider file `HomeCredit_train.csv`, which contains anonymized data shared from a financial institution (https://www.kaggle.com/c/home-credit-default-risk/).\n",
    "\n",
    "Each row is a loan.  See `HomeCredit_description.csv` for a description of the columns (note that we only have a subset of the columns).  The target variable (column `TARGET`) contains 1 if the client had issues repaying the loan, 0 otherwise.\n",
    "\n",
    "If you need it for faster experiments, the file  `HomeCredit_train_small.csv` contains only a small part of the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: exploratory analysis\n",
    "Open the dataset using Pandas.\n",
    "\n",
    "### 1.1\n",
    "Which fraction of the loans are not repayed? \n",
    "\n",
    "### 1.2\n",
    "Choose 3 variables you like and whose meaning you understand. Make one or two plots for each to describe its distribution (univariate analysis), and to check whether there is an obvious relation to the target variable (bivariate analysis)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.1 \n",
    "import pandas as pd\n",
    "\n",
    "#file = \"HomeCredit_test_blind.csv\"\n",
    "file = \"HomeCredit_train.csv\"\n",
    "#file = \"HomeCredit_train_small.csv\"\n",
    "\n",
    "df = pd.read_csv(file)\n",
    "m = df[\"TARGET\"].mean()\n",
    "print(f\"Fraction of not repayed loans = {m:.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.2\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "df_insolvment = df[df['TARGET']==1]\n",
    "\n",
    "l = go.Layout(\n",
    "    title=\"occupation type - relative frequency\",\n",
    "    yaxis={'title':'occupation type', 'autorange': 'reversed'},\n",
    "    xaxis={'title':'% of person'}\n",
    ")\n",
    "\n",
    "## Counts the occurrence of unqiue elements and stores in a series type\n",
    "occ_type = df['OCCUPATION_TYPE'].value_counts() \n",
    "f1 = go.Bar(x=occ_type.values/np.sum(occ_type)*100, y=occ_type.index, orientation='h')\n",
    "go.Figure(f1, l)\n",
    "\n",
    "\n",
    "#fig = make_subplots(shared_xaxes=True, rows=2, cols=1)\n",
    "#fig.add_trace(f1, row=1, col=1)\n",
    "#fig.add_trace(f2, row=2, col=1)\n",
    "#fig.show()\n",
    "#go.Figure([f1,f2], l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = go.Layout(\n",
    "    title=\"occupation type - % not of insolvment\",\n",
    "    yaxis={'title':'occupation type', 'autorange': 'reversed'},\n",
    "    xaxis={'title':'% of insolvment'}    \n",
    ")\n",
    "occ_type_insolv = df_insolvment['OCCUPATION_TYPE'].value_counts() \n",
    "f2 = go.Bar(x=occ_type_insolv.values/np.sum(occ_type_insolv)*100, y=occ_type_insolv.index, orientation='h', marker_color='rgba(255, 0, 0, 1)')\n",
    "go.Figure(f2, l)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#g = df.groupby('CODE_GENDER').count().index\n",
    "g = df['CODE_GENDER'].value_counts()\n",
    "g\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "gender = df['CODE_GENDER'].value_counts()\n",
    "l = go.Layout(\n",
    "    title=\"gender - relative frequency\",\n",
    "    showlegend=False,\n",
    "    #uniformtext={'minsize':9,'mode': 'hide'}\n",
    ")\n",
    "#f = go.Pie(values=gender.values, labels=gender.index, textposition='inside', textinfo='percent+label')\n",
    "f = go.Pie(values=gender.values, labels=gender.index, textinfo='percent+label')\n",
    "go.Figure(f, l)\n",
    "#s\n",
    "#df = pd.DataFrame({'FuncGroup':s.index, 'Count':s.values}) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_refused=df[df['TARGET']==1]\n",
    "df_accepted=df[df['TARGET']==0]\n",
    "y1 = df_refused.groupby(['OCCUPATION_TYPE'])['TARGET'].mean()\n",
    "y2 = df_accepted.groupby(['OCCUPATION_TYPE'])['TARGET'].mean()\n",
    "total = df.groupby(['OCCUPATION_TYPE'])['TARGET']\n",
    "#total\n",
    "y1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_refused=df[df['TARGET']==1]\n",
    "#df_accepted=df[df['TARGET']==0]\n",
    "y1 = df_refused.groupby(['OCCUPATION_TYPE']).count()['TARGET'].reset_index()\n",
    "#y2 = df_accepted.groupby(['OCCUPATION_TYPE']).count()['TARGET'].reset_index()\n",
    "y1.set_index('OCCUPATION_TYPE', inplace=True)\n",
    "#y2.set_index('OCCUPATION_TYPE', inplace=True)\n",
    "ciao = df.groupby(['OCCUPATION_TYPE']).count()['TARGET']\n",
    "\n",
    "fig = go.Figure(data=[\n",
    "    go.Bar(name='not paid', x=y1.index, y=(y1['TARGET']/ciao)*100)\n",
    " #   go.Bar(name='paid', x=y2.index, y=(y2['TARGET']/ciao)*100)\n",
    "])\n",
    "\n",
    "fig.update_layout(barmode='stack')\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: preparing a training and a validation dataset\n",
    "\n",
    "### 2.1\n",
    "Randomly split the 200k rows of your dataset in two groups; keep 150k rows for training and use 50k for validating your models.\n",
    "\n",
    "### 2.2\n",
    "Choose three variables that are already numeric.  Build the following numpy arrays:\n",
    "- `X_tr` (2 dimensions: 150k rows, 3 columns)\n",
    "- `y_tr` (1 dimension: 150k elements)\n",
    "- `X_val` (2 dimensions: 50k rows, 3 columns)\n",
    "- `y_val` (1 dimension: 150k elements)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: training and scoring simple models\n",
    "\n",
    "### 3.1\n",
    "Train a K-Nearest-Neighbors classifier (use the sklearn function) and compute its accuracy on the validation set.  Compare it with the accuracy of a classifier that always returns 0.  Comment.\n",
    "\n",
    "### 3.2\n",
    "Train the classifier with K=20, and use the `predict_proba(...)` function on the trained classifier to obtain a *score* for each instance in the validation set.  Consider the distribution of the scores returned for the instances of the validation set.\n",
    "\n",
    "Describe what is in this context the concept of TP, TN, FP, FN, TPR, FPR.\n",
    "\n",
    "Write a function that given a threshold, computes the number of TP, TN, FP, FN. \n",
    "\n",
    "How would you describe what the TPR and FPR are in this context?\n",
    "\n",
    "\n",
    "### 3.3\n",
    "Using your function defined above, compute the TPR and FPR for a large number of different thresholds.  Plot the ROC curve.\n",
    "\n",
    "Using the [appropriate sklearn function](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html), compute the AUC of your classifier.\n",
    "\n",
    "### 3.4\n",
    "Draw the ROC curve and compute the AUC value for two \"dummy\" classifiers:\n",
    "- one that always returns a score of 0 for each sample\n",
    "- one that returns a random score for each sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: training better models (optional)\n",
    "\n",
    "### 4.1\n",
    "Normalize the data and repeat the analysis. Is the accuracy better?\n",
    "\n",
    "### 4.2\n",
    "Try using other classifiers.  A good option is the [random forest classifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html).\n",
    "\n",
    "### 4.3\n",
    "Try using more numerical features.\n",
    "\n",
    "### 4.4\n",
    "There is a lot of information also in the categorical features. Find a way to use them in a classifier.  For example, you can use One Hot Encoding, implementing it manually or by using the [appropriate sklearn function](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html).\n",
    "\n",
    "### 4.5\n",
    "Given a classifier, study how the AUC on the validation data decreases if you use only part of the training data.  Make a plot with the AUC on the y and the fraction of training data on the x. Compare for example 0.1%, 1%, 10%, 100%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5\n",
    "Download the testing dataset `HomeCredit_test_blind.csv`. It does not contain the target variable. Predict it with your best classifier, and submit the results as a CSV file."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
